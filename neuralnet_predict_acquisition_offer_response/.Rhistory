# For Clinton
Net.vote<-neuralnet(Clinton ~ ., hidden=c(1,1), data=train_data, act.fct='logistic',
linear.output=TRUE)
plot(Net.vote)
#Evaluate neural network on the training set
predictions<-predict(Net.vote, train_data)
#Convert the data to "unnormalized"
unscalepred<- predictions*(max(train_data$Clinton)-min(train_data$Clinton) )+min(train_data$Clinton)
#MAE
mean(abs(unscalepred-train_data$Clinton))
#CORRELATION
cor(train_data$Clinton, unscalepred)
# For Clinton
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(1,1), data=train_data, act.fct='logistic',
linear.output=TRUE)
plot(Net.vote)
#Evaluate neural network on the training set
predictions<-predict(Net.vote, train_data)
#Evaluate neural network on the training set
predictions<-predict(Net.vote, train_data)
#Convert the data to "unnormalized"
unscalepred<- predictions*(max(train_data$Clinton)-min(train_data$Clinton) )+min(train_data$Clinton)
#MAE
mean(abs(unscalepred-train_data$Clinton))
#CORRELATION
cor(train_data$Clinton, unscalepred)
#Run model on test dataset
predictions<-predict(Net.vote,test_data)
#Get the result and unscale it
unscalepred<- predictions*(max(test_data$Clinton)-min(test_data$Clinton) )+min(test_data$Clinton)
#MAE
mean(abs(unscalepred-test_data$Clinton))
#CORRELATION
cor(test_data$Clinton, unscalepred)
# For Clinton
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(2,2), data=train_data, act.fct='logistic')
# For Clinton
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(2,2), data=train_data, act.fct='linear')
# For Clinton
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(2,2), data=train_data, act.fct='lin')
# For Clinton
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(2,2), data=train_data, act.fct="linear")
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(2,2), data=train_data, act.fct="logistic",
stepmax=1e6)
plot(Net.vote)
#Evaluate neural network on the training set
predictions<-predict(Net.vote, train_data)
#Convert the data to "unnormalized"
unscalepred<- predictions*(max(train_data$Clinton)-min(train_data$Clinton) )+min(train_data$Clinton)
#MAE
mean(abs(unscalepred-train_data$Clinton))
#CORRELATION
cor(train_data$Clinton, unscalepred)
#Run model on test dataset
predictions<-predict(Net.vote,test_data)
#Get the result and unscale it
unscalepred<- predictions*(max(test_data$Clinton)-min(test_data$Clinton) )+min(test_data$Clinton)
#MAE
mean(abs(unscalepred-test_data$Clinton))
#CORRELATION
cor(test_data$Clinton, unscalepred)
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(2,2), data=train_data, act.fct="logistic",
stepmax=1e6, linear.output = TRUE)
plot(Net.vote)
#Evaluate neural network on the training set
predictions<-predict(Net.vote, train_data)
#Convert the data to "unnormalized"
unscalepred<- predictions*(max(train_data$Clinton)-min(train_data$Clinton) )+min(train_data$Clinton)
#MAE
mean(abs(unscalepred-train_data$Clinton))
#CORRELATION
cor(train_data$Clinton, unscalepred)
#Run model on test dataset
predictions<-predict(Net.vote,test_data)
#Evaluate neural network on the training set
predictions<-predict(Net.vote, train_data)
#Convert the data to "unnormalized"
unscalepred<- predictions*(max(train_data$Clinton)-min(train_data$Clinton) )+min(train_data$Clinton)
#MAE
mean(abs(unscalepred-train_data$Clinton))
#CORRELATION
cor(train_data$Clinton, unscalepred)
#Run model on test dataset
predictions<-predict(Net.vote,test_data)
#Get the result and unscale it
unscalepred<- predictions*(max(test_data$Clinton)-min(test_data$Clinton) )+min(test_data$Clinton)
#MAE
mean(abs(unscalepred-test_data$Clinton))
#CORRELATION
cor(test_data$Clinton, unscalepred)
# For Trump
Net.vote<-neuralnet(Trump ~ . - Clinton, hidden=c(2,2), data=train_data, act.fct="logistic",
stepmax=1e6, linear.output = TRUE)
plot(Net.vote)
Net.vote<-neuralnet(Trump ~ . - Clinton, hidden=c(2,2), data=train_data, act.fct="logistic",
stepmax=1e6, linear.output = TRUE)
# For Clinton
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(3,2), data=train_data, act.fct="logistic",
stepmax=1e6, linear.output = TRUE)
plot(Net.vote)
#Evaluate neural network on the training set
predictions<-predict(Net.vote, train_data)
#Convert the data to "unnormalized"
unscalepred<- predictions*(max(train_data$Clinton)-min(train_data$Clinton) )+min(train_data$Clinton)
#MAE
mean(abs(unscalepred-train_data$Clinton))
#CORRELATION
cor(train_data$Clinton, unscalepred)
#Run model on test dataset
predictions<-predict(Net.vote,test_data)
#Get the result and unscale it
unscalepred<- predictions*(max(test_data$Clinton)-min(test_data$Clinton) )+min(test_data$Clinton)
#MAE
mean(abs(unscalepred-test_data$Clinton))
#CORRELATION
cor(test_data$Clinton, unscalepred)
# For Clinton
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(4,2), data=train_data, act.fct="logistic",
stepmax=1e6, linear.output = TRUE)
plot(Net.vote)
#Evaluate neural network on the training set
predictions<-predict(Net.vote, train_data)
#Convert the data to "unnormalized"
unscalepred<- predictions*(max(train_data$Clinton)-min(train_data$Clinton) )+min(train_data$Clinton)
#MAE
mean(abs(unscalepred-train_data$Clinton))
#CORRELATION
cor(train_data$Clinton, unscalepred)
#Run model on test dataset
predictions<-predict(Net.vote,test_data)
#Get the result and unscale it
unscalepred<- predictions*(max(test_data$Clinton)-min(test_data$Clinton) )+min(test_data$Clinton)
#MAE
mean(abs(unscalepred-test_data$Clinton))
#CORRELATION
cor(test_data$Clinton, unscalepred)
# For Clinton
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(4,3), data=train_data, act.fct="logistic",
stepmax=1e6, linear.output = TRUE)
# For Clinton
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(4,2), data=train_data, act.fct="logistic",
stepmax=1e6, linear.output = TRUE)
Net.vote<-neuralnet(Clinton ~ . -Trump, hidden=c(3,2), data=train_data, act.fct="logistic",
stepmax=1e6, linear.output = TRUE)
plot(Net.vote)
#Evaluate neural network on the training set
predictions<-predict(Net.vote, train_data)
#Convert the data to "unnormalized"
unscalepred<- predictions*(max(train_data$Clinton)-min(train_data$Clinton) )+min(train_data$Clinton)
#MAE
mean(abs(unscalepred-train_data$Clinton))
#CORRELATION
cor(train_data$Clinton, unscalepred)
#Run model on test dataset
predictions<-predict(Net.vote,test_data)
#Get the result and unscale it
unscalepred<- predictions*(max(test_data$Clinton)-min(test_data$Clinton) )+min(test_data$Clinton)
#MAE
mean(abs(unscalepred-test_data$Clinton))
#CORRELATION
cor(test_data$Clinton, unscalepred)
Net.vote<-neuralnet(Trump ~ . - Clinton, hidden=c(3,2), data=train_data, act.fct="logistic",
stepmax=1e6, linear.output = TRUE)
plot(Net.vote)
#Evaluate neural network on the training set
predictions<-predict(Net.vote, train_data)
#Convert the data to "unnormalized"
unscalepred<- predictions*(max(train_data$Trump)-min(train_data$Trump) )+min(train_data$Trump)
#MAE
mean(abs(unscalepred-train_data$Trump))
#CORRELATION
cor(train_data$Trump, unscalepred)
#Run model on test dataset
predictions<-predict(Net.vote,test_data)
#Get the result and unscale it
unscalepred<- predictions*(max(test_data$Trump)-min(test_data$Trump) )+min(test_data$Trump)
#MAE
mean(abs(unscalepred-test_data$Trump))
#CORRELATION
cor(test_data$Trump, unscalepred)
plot(x=maxmindf$Bachlorsorhigher, y=maxmindf$Trump, col = "red",
xlab='Bachlorsorhigher',ylab='Trump/Clinton fraction')
par(new=TRUE)
plot(x=maxmindf$Bachlorsorhigher, y=maxmindf$Clinton, col = "blue",
xlab='Bachlorsorhigher',ylab='Trump/Clinton fraction')
legend("top",
legend=c("Trump","Clinton"),
col=c("red","blue"),
lty=1,lwd=2)
install.packages("corrplot", type = "binary")
install.packages("psych", type = "binary")
library(corrplot)
library(psych)
library(ggplot2)
library(ggplot2)
library(psych)
library(corrplot)
# Dataset is loaded
frame<-read.csv("/Users/chiaentsai/Desktop/Data_Analytics/Data_Analytics_Code/hw8_2/DC_PropertieResidentialunder1mill.csv")
summary(frame)
# check the dimension of the frame
dim(frame)
# Dataset is loaded
frame<-read.csv("/Users/chiaentsai/Desktop/Data_Analytics/Data_Analytics_Code/hw8_2/DC_PropertieResidentialunder1mill.csv")
summary(frame) #LOG.PRICE or PRICE need to be taken out of the final model as they are the target variable
# check the dimension of the frame
dim(frame)
# Distribution of some of the variables (one-hot encoded ones are not represented)
multi.hist(x = frame[, 1:12], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
multi.hist(x = frame[, 13:24], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
# Distribution of some of the variables (one-hot encoded ones are not represented)
multi.hist(x = frame[, 1:12], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
frame
# Distribution of some of the variables (one-hot encoded ones are not represented)
multi.hist(x = frame[, 1:12], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
# Some scatterplots are obtained
ggplot(frame, aes(x=ROOMS, y=PRICE)) + geom_point()
ggplot(frame, aes(x=STORIES, y=PRICE)) + geom_point()
ggplot(frame, aes(x=EYB, y=PRICE)) + geom_point()
ggplot(frame, aes(x=GrossBuildingArea, y=PRICE)) + geom_point()
ggplot(frame, aes(x=LANDAREA, y=PRICE)) + geom_point()
library(corrplot)
library(psych)
library(ggplot2)
# Dataset is loaded
frame<-read.csv("/Users/chiaentsai/Desktop/Data_Analytics/Data_Analytics_Code/hw8_2/DC_PropertieResidentialunder1mill.csv")
# Distribution of some of the variables (one-hot encoded ones are not represented)
multi.hist(x = frame[, 1:12], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
# Distribution of some of the variables (one-hot encoded ones are not represented)
multi.hist(x = frame[, 1:12], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
multi.hist(x = frame[, 13:24], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
# Distribution of some of the variables (one-hot encoded ones are not represented)
multi.hist(x = frame[, 1:12], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
multi.hist(x = frame[, 13:24], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
# Some scatterplots are obtained
ggplot(frame, aes(x=ROOMS, y=PRICE)) + geom_point()
ggplot(frame, aes(x=STORIES, y=PRICE)) + geom_point()
# Dataset is loaded
frame<-read.csv("/Users/chiaentsai/Desktop/Data_Analytics/Data_Analytics_Code/hw8_2/DC_PropertieResidentialunder1mill.csv")
summary(frame)
# Some scatterplots are obtained
ggplot(frame, aes(x=ROOMS, y=PRICE)) + geom_point()
ggplot(frame, aes(x=STORIES, y=PRICE)) + geom_point()
ggplot(frame, aes(x=EYB, y=PRICE)) + geom_point()
ggplot(frame, aes(x=YearSold, y=PRICE)) + geom_point()
ggplot(frame, aes(x=LANDAREA, y=PRICE)) + geom_point()
ggplot(frame, aes(x=GrossBuildingArea, y=PRICE)) + geom_point()
ggplot(frame, aes(x=YearSold, y=PRICE)) + geom_point()
frame
m
ggplot(frame, aes(x=LANDAREA, y=PRICE)) + geom_point()
# Log Price will be used to see if we obtain better insights
ggplot(frame, aes(x=ROOMS, y=logPrice)) + geom_point()
ggplot(frame, aes(x=STORIES, y=logPrice)) + geom_point()
ggplot(frame, aes(x=EYB, y=logPrice)) + geom_point()
ggplot(frame, aes(x=YearSold, y=logPrice)) + geom_point()
ggplot(frame, aes(x=GrossBuildingArea, y=logPrice)) + geom_point()
ggplot(frame, aes(x=LANDAREA, y=logPrice)) + geom_point()
# Log Price will be used to see if we obtain better insights
ggplot(frame, aes(x=ROOMS, y=logPrice)) + geom_point()
ggplot(frame, aes(x=STORIES, y=logPrice)) + geom_point()
ggplot(frame, aes(x=GrossBuildingArea, y=logPrice)) + geom_point()
ggplot(frame, aes(x=LANDAREA, y=logPrice)) + geom_point()
# Eliminating NA
dim(frame)
# remove empty rows
frame <- na.omit(frame)
dim(frame)
frame_init <- frame
# Eliminating columns where all values are the same
summary(frame_init)
frame_init$Single <-  NULL
frame_init$Residential_0.Condo_1 <-  NULL
frame_init$MassachusettsAvenueHeights <-  NULL
frame_init$Woodley <-  NULL
frame_init$Kalorama <-  NULL
frame_init$ASSESSMENT_NBHD <-  NULL
frame_init$QUADRANT <-  NULL
# Some categorical features are already broken down into one-hot encodings. They will be eliminated as well.
frame_init$ASSESSMENT_NBHD <-  NULL
frame_init$QUADRANT <-  NULL
# Some other categorical features have too many classes to be interpreted with one-hot encoding.
# Initially, they will be eliminated altough other external information could be mapped to substitute them
# A possible example is neighbourhood rent per capita or crime rate.
# Furthermore, some of the information is doubled in the dataset so it can be assumed that a few of the features
# do not provide much additional information and can contribute to overfitting.
frame_init$ASSESSMENT_SUBNBHD <-  NULL
frame_init$CENSUS_BLOCK <-  NULL
frame_init$SQUARE <-  NULL
# Computing Correlation matrix.
# Only selecting columns that are numeric variables.
nums <- unlist(lapply(frame_init, is.numeric))
frame_numeric <- frame_init[ , nums]
summary(frame_numeric)
# Computing Correlation matrix.
# Only selecting columns that are numeric variables.
nums <- unlist(lapply(frame_init, is.numeric))
frame_numeric <- frame_init[ , nums]
summary(frame_numeric)
frame_for_corr <- frame_numeric[,c("BATHROOMS", "ROOMS", "EYB", "YearSold", "PRICE", "GrossBuildingArea", "LANDAREA")]
#Look at the correlation matrix for all variables in this data set
cor(frame_for_corr, method="pearson")
#Draw the correlation graph
corrplot.mixed(corr=cor(frame_for_corr,
method="pearson"), tl.pos="lt", tl.srt=45,
addCoef.col = "black")
# Take out log.price as we will be predicting PRICE
frame_init$logPrice <-  NULL
summary(frame_init)
# Deep learning
library(corrplot)
library(psych)
library(ggplot2)
# Dataset is loaded
frame<-read.csv("/Users/chiaentsai/Desktop/Data_Analytics/Data_Analytics_Code/hw8_2/DC_PropertieResidentialunder1mill.csv")
summary(frame)
# check the dimension of the frame
dim(frame)
# Distribution of some of the variables (one-hot encoded ones are not represented)
multi.hist(x = frame[, 1:12], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
multi.hist(x = frame[, 13:24], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
# Eliminating NA
dim(frame)
# remove empty rows
frame <- na.omit(frame)
dim(frame)
frame_init <- frame
# Eliminating columns where all values are the same
summary(frame_init)
frame_init$Single <-  NULL
frame_init$Residential_0.Condo_1 <-  NULL
frame_init$MassachusettsAvenueHeights <-  NULL
frame_init$Woodley <-  NULL
frame_init$Kalorama <-  NULL
# Some categorical features are already broken down into one-hot encodings. They will be eliminated as well.
frame_init$ASSESSMENT_NBHD <-  NULL
frame_init$QUADRANT <-  NULL
# Dataset is loaded
frame<-read.csv("/Users/chiaentsai/Desktop/Data_Analytics/Data_Analytics_Code/hw8_2/DC_PropertieResidentialunder1mill.csv")
summary(frame)
# check the dimension of the frame
dim(frame)
# Distribution of some of the variables (one-hot encoded ones are not represented)
multi.hist(x = frame[, 1:12], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
multi.hist(x = frame[, 13:24], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
# Distribution of some of the variables (one-hot encoded ones are not represented)
multi.hist(x = frame[, 1:12], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
# Some scatterplots are obtained
ggplot(frame, aes(x=ROOMS, y=PRICE)) + geom_point()
# Eliminating NA
dim(frame)
frame_init <- frame
# Eliminating columns where all values are the same
summary(frame_init)
# Eliminating columns where all values are the same
summary(frame_init)
frame_init$Single <-  NULL
frame_init$Residential_0.Condo_1 <-  NULL
frame_init$MassachusettsAvenueHeights <-  NULL
frame_init$Woodley <-  NULL
frame_init$Kalorama <-  NULL
dim(frame)
# Eliminating columns where all values are the same
summary(frame_init)
frame_init$Single <-  NULL
frame_init$Residential_0.Condo_1 <-  NULL
frame_init$MassachusettsAvenueHeights <-  NULL
frame_init$Woodley <-  NULL
frame_init$Kalorama <-  NULL
dim(frame)
frame
# Some categorical features are already broken down into one-hot encodings. They will be eliminated as well.
frame_init$ASSESSMENT_NBHD <-  NULL
frame_init$QUADRANT <-  NULL
dim(frame)
# Some other categorical features have too many classes to be interpreted with one-hot encoding.
# Initially, they will be eliminated altough other external information could be mapped to substitute them
# A possible example is neighbourhood rent per capita or crime rate.
# Furthermore, some of the information is doubled in the dataset so it can be assumed that a few of the features
# do not provide much additional information and can contribute to overfitting.
frame_init$ASSESSMENT_SUBNBHD <-  NULL
frame_init$CENSUS_BLOCK <-  NULL
frame_init$SQUARE <-  NULL
dim(frame_init)
# Computing Correlation matrix.
# Only selecting columns that are numeric variables.
nums <- unlist(lapply(frame_init, is.numeric))
frame_numeric <- frame_init[ , nums]
summary(frame_numeric)
frame_for_corr <- frame_numeric[,c("BATHROOMS", "ROOMS", "EYB", "YearSold", "PRICE", "GrossBuildingArea", "LANDAREA")]
#Look at the correlation matrix for all variables in this data set
cor(frame_for_corr, method="pearson")
#Draw the correlation graph
corrplot.mixed(corr=cor(frame_for_corr,
method="pearson"), tl.pos="lt", tl.srt=45,
addCoef.col = "black")
# Deep learning
library(corrplot)
library(psych)
library(ggplot2)
# Dataset is loaded
frame<-read.csv("/Users/chiaentsai/Desktop/Data_Analytics/Data_Analytics_Code/hw8_2/DC_PropertieResidentialunder1mill.csv")
summary(frame)
# check the dimension of the frame
dim(frame)
# Distribution of some of the variables (one-hot encoded ones are not represented)
multi.hist(x = frame[, 1:12], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
multi.hist(x = frame[, 13:24], dcol= c("blue", "red"), dlty = c("dotted", "solid")  )
# Some scatterplots are obtained
ggplot(frame, aes(x=ROOMS, y=PRICE)) + geom_point()
# Eliminating NA
dim(frame)
# remove empty rows
frame <- na.omit(frame)
dim(frame)
frame_init <- frame
# Eliminating columns where all values are the same
summary(frame_init)
frame_init$Single <-  NULL
frame_init$Residential_0.Condo_1 <-  NULL
frame_init$MassachusettsAvenueHeights <-  NULL
frame_init$Woodley <-  NULL
frame_init$Kalorama <-  NULL
frame
# Some categorical features are already broken down into one-hot encodings. They will be eliminated as well.
frame_init$ASSESSMENT_NBHD <-  NULL
frame_init$QUADRANT <-  NULL
# Some other categorical features have too many classes to be interpreted with one-hot encoding.
# Initially, they will be eliminated altough other external information could be mapped to substitute them
# A possible example is neighbourhood rent per capita or crime rate.
# Furthermore, some of the information is doubled in the dataset so it can be assumed that a few of the features
# do not provide much additional information and can contribute to overfitting.
frame_init$ASSESSMENT_SUBNBHD <-  NULL
frame_init$CENSUS_BLOCK <-  NULL
frame_init$SQUARE <-  NULL
# Computing Correlation matrix.
# Only selecting columns that are numeric variables.
nums <- unlist(lapply(frame_init, is.numeric))
frame_numeric <- frame_init[ , nums]
summary(frame_numeric)
frame_for_corr <- frame_numeric[,c("BATHROOMS", "ROOMS", "EYB", "YearSold", "PRICE", "GrossBuildingArea", "LANDAREA")]
#Look at the correlation matrix for all variables in this data set
cor(frame_for_corr, method="pearson")
#Draw the correlation graph
corrplot.mixed(corr=cor(frame_for_corr,
method="pearson"), tl.pos="lt", tl.srt=45,
addCoef.col = "black")
summary(frame_numeric)
frame_for_corr <- frame_numeric[,c("BATHROOMS", "ROOMS", "EYB", "YearSold", "PRICE", "GrossBuildingArea", "LANDAREA")]
#Look at the correlation matrix for all variables in this data set
cor(frame_for_corr, method="pearson")
#Draw the correlation graph
corrplot.mixed(corr=cor(frame_for_corr,
method="pearson"), tl.pos="lt", tl.srt=45,
addCoef.col = "black")
# Take out log.price as we will be predicting PRICE
frame_init$logPrice <-  NULL
summary(frame_init)
# Only the numeric columns will be normalized. The original dataframe is split into two
nums <- unlist(lapply(frame_init, is.numeric))
# Only the numeric columns will be normalized. The original dataframe is split into two
nums <- unlist(lapply(frame_init, is.numeric))
frame_numeric <- frame_init[ , nums]
frame_non_numeric <- frame_init[ , !nums]
#check if number of rows of the splitted datasets remain the same
dim(frame_non_numeric)[2] + dim(frame_numeric)[2] == dim(frame_init)[2]
# Data is normalized relative to the whole dataset (before doing the train-test split)
normalize<-function(x) {return ((x-min(x))/(max(x)-min(x)))}
# Normalize data except the first column which is property ID
frame_norm<-as.data.frame(lapply(frame_numeric[,-1], normalize))
summary(frame_norm)
# Combine the normalized numeric/non-numeric datasets
# First column will be reintroduced
# First column is previously removed when normalization
frame <- cbind(X.1=frame_numeric$X.1, frame_norm, frame_non_numeric)
# check the dimensionality remain the same
dim(frame)[2]== dim(frame_init)[2]
# Test and training extracted
ind<-sample(1:nrow(frame), 0.7*nrow(frame))
train_data_counties <- frame[ind, "X.1"]
test_data_counties <- frame[-ind, "X.1"]
train_data<- frame[ind,2:ncol(frame)]
test_data<- frame[-ind,2:ncol(frame)]
# Create data set for analysis with LIME
for_lime<-sample(1:nrow(train_data), 5) # Pick 5 indices from the training set
data_for_lime<-train_data[for_lime,]
print(data_for_lime)
install.packages("h2o", type = "binary")
install.packages("lime", type = "binary")
library(lime)
library(h2o)
# Fit a deep neural network with 2 hidden layers
h2o.init()
train_data<-as.h2o(train_data) # create training h2o data frame
test_data<-as.h2o(test_data) # use the remaining data for testing
?h2o.deeplearning
all_variables <- colnames(train_data)
input_variables <- all_variables[all_variables != "PRICE"]
setdiff(all_variables, input_variables)
hyper_params <- list(
activation=c("Maxout", "Tanh"),
hidden = list(c(2,2), c(3,2), c(3,3)),
epochs=c(30,100,500),
l1=c(0,.01),
adaptive_rate=c(TRUE,FALSE))
dl_grid <- h2o.grid(algorithm = "deeplearning",
x=input_variables,
y = "PRICE",
training_frame = train_data,
nfolds=10,
standardize=TRUE,
hyper_params = hyper_params)
